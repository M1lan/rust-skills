---
name: rust-distributed
description: "Distributed systems: Raft, 2PC, consensus algorithms, consistency and coordination"
globs: ["**/*.rs"]
---

# Distributed Systems

## Core issues

Key question: How do we balance consistency and availability in distributed
systems?

Distributed systems require CAP trade-offs.

---

## Raft Consensus algorithm

### Raft Core Concept

```text
┌─────────────────────────────────────────────────────┐
│ Raft Cluster                                            │
├─────────────────────────────────────────────────────┤
│                                                         │
│ ┌─────────┐      ┌─────────┐      ┌─────────┐        │
│ │ Leader   │ ◄──► │ Follower │ ◄──► │ Follower │        │
│ │ Nodes    │      │ Nodes    │      │ Nodes   │        │
│ └────┬────┘      └─────────┘      └─────────┘        │
│       │                                                 │
│ - Processing client requests                            │
│ - Copy Log To Follower                                  │
│ - Managing heartbeats and elections                     │
└─────────────────────────────────────────────────────┘
```

### Status machine

```rust
// Raft Node Status
enum RaftState {
 Follower,
 Candidate,
 Leader,
}

struct RaftNode {
 state: RaftState,
 current_term: u64,
 voted_for: Option<u64>,
 log: Vec<LogEntry>,
 commit_index: usize,
 last_applied: usize,

 // Election-related
 election_timeout: Duration,
 last_heartbeat: Instant,

 // Cluster Configuration
 node_id: u64,
 peers: Vec<u64>,
}
```

### Log Copy

```rust
struct LogEntry {
 term: u64,
 index: usize,
 command: Vec<u8>,
}

impl RaftNode {
 // Leader Copy Log To Follower
 fn replicate_log(&mut self, peer: u64) {
 let prev_log_index = self.get_last_log_index_for(peer);
 let prev_log_term = self.get_last_log_term_for(peer);

 let entries: Vec<LogEntry> = self.log
 [(prev_log_index + 1)..]
 .to_vec();

 let rpc = AppendEntriesRequest {
 term: self.current_term,
 leader_id: self.node_id,
 prev_log_index,
 prev_log_term,
 entries,
 leader_commit: self.commit_index,
 };

 self.send_append_entries(peer, rpc);
 }
}
```

### Electoral mechanisms

```rust
impl RaftNode {
 fn start_election(&mut self) {
 self.state = RaftState::Candidate;
 self.current_term += 1;
 self.voted_for = Some(self.node_id);

 let mut votes = 1;

 // Request to vote for all nodes
 for peer in &self.peers {
 let request = RequestVoteRequest {
 term: self.current_term,
 candidate_id: self.node_id,
 last_log_index: self.log.len(),
 last_log_term: self.get_last_log_term(),
 };

 if let Some(response) = self.send_request_vote(peer, request) {
 if response.vote_granted {
 votes += 1;
 if votes > self.peers.len() / 2 {
 self.become_leader();
 return;
 }
 }
 }
 }

 // Elections failed,Back Follower
 self.state = RaftState::Follower;
 }
}
```

---

## Two-stage submission (2PC)

### Coordinator

```rust
struct TwoPhaseCommitCoordinator {
 transaction_id: u128,
 participants: Vec<Participant>,
 state: TwoPCState,
}

enum TwoPCState {
 Init,
 WaitingPrepare,
 WaitingCommit,
 Committed,
 Aborted,
}

impl TwoPhaseCommitCoordinator {
 pub fn start_transaction(&mut self) {
 self.state = TwoPCState::WaitingPrepare;

 // Phase I:Send prepare
 for participant in &self.participants {
 participant.send(PrepareMessage {
 transaction_id: self.transaction_id,
 });
 }
 }

 pub fn handle_prepare_response(&mut self, response: PrepareResponse) {
 if response.vote == Vote::Abort {
 self.abort();
 } else if self.all_prepared() {
 self.state = TwoPCState::WaitingCommit;

 // Phase II:Send commit
 for participant in &self.participants {
 participant.send(CommitMessage {
 transaction_id: self.transaction_id,
 });
 }
 }
 }
}
```

### Participants

```rust
struct Participant {
 transaction_manager: TransactionManager,
 state: ParticipantState,
}

enum ParticipantState {
 Init,
 Prepared,
 Committed,
 Aborted,
}

impl Participant {
 pub fn handle_prepare(&mut self, msg: PrepareMessage) {
 // Do Local Operations
 let result = self.transaction_manager.execute();

 match result {
 Ok(_) => {
 self.state = ParticipantState::Prepared;
 self.send(PrepareResponse {
 vote: Vote::Commit,
 ..msg
 });
 }
 Err(_) => {
 self.send(PrepareResponse {
 vote: Vote::Abort,
 ..msg
 });
 }
 }
 }
}
```

### 2PC Problems and Solutions

| Problem              | Reason                       | Solutions                                |
|----------------------|------------------------------|------------------------------------------|
| Blocking             | Coordinator failure          | Timeouts, backup coordinator             |
| Single point failure | Reliance on coordinator      | Distributed coordinator (etcd/ZooKeeper) |
| Performance          | Multiple network round trips | Batch submission, tuned timeouts         |

---

## Distributed consistency models

```rust
// Eventual consistency
trait EventuallyConsistent {
 fn put(&self, key: &str, value: &str);
 fn get(&self, key: &str) -> Option<String>;
}

// Strong consistency (linearizability)
trait Linearizable {
 fn put(&self, key: &str, value: &str) -> Result<()>;
 fn get(&self, key: &str) -> Result<String>;
}

// Sequential consistency
trait SequentialConsistent {
 fn put(&self, key: &str, value: &str);
 fn get(&self, key: &str) -> Vec<String>; // Return history version
}
```

---

## Distributed ID generation

```rust
// Snowflake algorithm
struct SnowflakeGenerator {
 worker_id: u64,
 datacenter_id: u64,
 sequence: u64,
 last_timestamp: u64,
}

impl SnowflakeGenerator {
 pub fn generate(&mut self) -> u64 {
 let timestamp = current_timestamp();

 if timestamp == self.last_timestamp {
 self.sequence = (self.sequence + 1) & 0xFFF; // 12bit
 if self.sequence == 0 {
 // Waiting for the next millisecond
 while current_timestamp() == timestamp {}
 }
 } else {
 self.sequence = 0;
 }

 self.last_timestamp = timestamp;

 (timestamp << 22) // 41-bit timestamp
 | (self.datacenter_id << 17) // 5-bit datacenter
 | (self.worker_id << 12) // 5-bit worker node
 | self.sequence // 12-bit sequence number
 }
}
```

---

## Distributed lock

```rust
use std::sync::{Arc, atomic::{AtomicU64, Ordering}};
use std::time::Duration;

struct DistributedLock {
 key: String,
 ttl: Duration,
 owner: u64,
}

impl DistributedLock {
 // Based on etcd distributed locks
 pub async fn try_lock(&self, owner: u64, ttl: Duration) -> Result<bool, LockError> {
 let response = etcd_client.put(
 format!("/lock/{}", self.key),
 owner.to_string(),
 Some(PutOptions::new().with_ttl(ttl))
 ).await?;

 // If no previous key, lock acquired.
 Ok(response.prev_key().is_none())
 }

 pub async fn unlock(&self, owner: u64) -> Result<(), LockError> {
 // Only by the lockholder.
 let response = etcd_client.get(format!("/lock/{}", self.key)).await?;

 if response.value() == owner.to_string() {
 etcd_client.delete(format!("/lock/{}", self.key)).await?;
 }

 Ok(())
 }
}
```

---

## Distributed event tracing

```rust
// Event Trace Mode
trait EventSourced {
 type Event;

 fn apply(&mut self, event: Self::Event);
 fn snapshot(&self) -> Self;
}

struct Aggregate {
 version: u64,
 events: Vec<Event>,
 state: AggregateState,
}

impl Aggregate {
 pub fn new() -> Self {
 Self {
 version: 0,
 events: Vec::new(),
 state: AggregateState::Init,
 }
 }

 pub fn apply_event(&mut self, event: Event) {
 self.state.transition(&event);
 self.events.push(event);
 self.version += 1;
 }

 pub fn snapshot(&self) -> EventSourcedSnapshot {
 EventSourcedSnapshot {
 version: self.version,
 state: self.state.clone(),
 }
 }
}
```

---

## Common problems

| Problem                 | Reason                      | Solve                          |
|-------------------------|-----------------------------|--------------------------------|
| Brain fracture.         | Network partition           | Quorum, tenure mechanism       |
| Live lock.              | Elections overtime conflict | Random timeout                 |
| Data inconsistencies    | Also Write                  | Conflict resolution strategies |
| Performance bottlenecks | Single-writer bottleneck    | Sharding, replication          |

---

## Links to other skills

```text
rust-distributed
 │
 ├─► rust-concurrency → Co-control
 ├─► rust-performance → Performance optimization
 └─► rust-async → Fabulous communications
```
